{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpz4PPJIdgXenARgTugQBT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3HIbY9Dt3l0","executionInfo":{"status":"ok","timestamp":1678942185402,"user_tz":420,"elapsed":152,"user":{"displayName":"Kristian Rascon","userId":"13766589462812670013"}},"outputId":"c5eaf681-a48d-44eb-d9af-ec157bce1105"},"outputs":[{"output_type":"stream","name":"stdout","text":["4.9999916604975585\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","#########################################################################################\n","################################### L2 regularization ###################################\n","#########################################################################################\n","\n","# we reuse the network in single perceptron chapter\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc = nn.Linear(1, 1) \n","\n","    def forward(self, x):\n","        out = self.fc(x)\n","        return out\n","\n","net = Net()\n","\n","# We want to use L2 regularization, it can be implemented with one extra argument called \"weight decay\" when initializing the optimizer.\n","# You may wonder why it is called weight decay instead of L2 penalty, they are the same concept but from different angle.\n","# We want to set weight decay as 0.005\n","l2_optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=0.005) # TO DO !!!!! think about what is missing and what might be wrong here\n","\n","\n","#########################################################################################\n","################################### Gradient Clipping ###################################\n","#########################################################################################\n","\n","batch_size, dim_in, dim_h, dim_out = 128, 2000, 200, 20 \n","learning_rate = 1e-4\n","\n","input_X = torch.randn(batch_size, dim_in)\n","output_Y = torch.randn(batch_size, dim_out)\n","\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(dim_in, dim_h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(dim_h, dim_out),\n",")\n","\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# we just update one step in our simple example\n","for values in range(1):\n","    optimizer.zero_grad()\n","    pred_y = model(input_X)\n","    loss = loss_fn(pred_y, output_Y)\n","    loss.backward()\n","    # we clip the gradient here, we want to clip norm to be no greater than 5\n","    # https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_\n","    torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=5, norm_type=2.0) # TO DO !!!!! think about what is missing and what might be wrong here\n","    optimizer.step()\n","\n","    # let's see if the norm of gradient is indeed clipped\n","    total_norm = 0.0\n","    for p in model.parameters():\n","        param_norm = p.grad.detach().data.norm(2)\n","        total_norm += param_norm.item() ** 2\n","    total_norm = total_norm ** 0.5\n","    print(total_norm)"]}]}