{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYhrzOT+RqzarB1NSnojV9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkwPaxvYCbJK","executionInfo":{"status":"ok","timestamp":1677521945188,"user_tz":420,"elapsed":15116,"user":{"displayName":"Kristian Rascon","userId":"13766589462812670013"}},"outputId":"f66f094f-527e-41c3-a81e-e2afb6b5e3c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 - loss: 19.788246481064178\n","Epoch 2 - loss: 2.088566152076649\n","Epoch 3 - loss: 1.0348039042996893\n","Epoch 4 - loss: 0.6779713097817077\n","Epoch 5 - loss: 0.5111734809527793\n","Epoch 6 - loss: 0.41908730364903457\n","Epoch 7 - loss: 0.3608735586934626\n","Epoch 8 - loss: 0.3238214458987701\n","Epoch 9 - loss: 0.299419405328346\n","Epoch 10 - loss: 0.27974825038370343\n"]}],"source":["'''\n","We will use a sinple multi-layer perceptron network in this lab to learn a quadratic function X^2 = Y` where `x` is a single dimension input variable and Y is a single dimension target variable.  \n","Name this file mlp.py.\n","Many details are illustrated in slp.py so they are not re-illustrated here.\n","'''\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","torch.manual_seed(42)\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        # Here you will need to define two single-layer perceptron (with non-linearality) to form a simple multi-layer perceptron network\n","        # Input is 1 dimension, we will make intermediate dimension 2, output is also single dimension of course. Below is the network:\n","        # X (1 dimension) -> 10 dimensional internal layer -> Relu -> Y (1 dimension)\n","        # F.relu: https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html\n","        self.fc1 = nn.Linear(1 , 10) # TO DO !!!!!\n","        self.fc2 = nn.Linear(10 ,1 ) # TO DO !!!!!\n","\n","    def forward(self, x):\n","        out = self.fc1( x) # TO DO !!!!!\n","        out = F.relu( out) # TO DO !!!!!\n","        out = self.fc2( out) # TO DO !!!!!\n","\n","        return out# TO DO !!!!!\n","\n","\n","net = Net()\n","\n","# let still use SGD but with learning rate of 0.05\n","optimizer = optim.SGD(net.parameters(), lr=0.05) # TO DO !!!!!\n","\n","# creating the data, notice we created 500 training data for linear function, but 1000 for quadratic function since quadratice function is obviously a little trickier to learn\n","# but still size of 1000 training data is very very small \n","X = 2 * torch.rand(1000) - 1 # X range is [-1, 1]\n","Y = X ** 2\n","\n","# train 10 epochs\n","for epoch in range( 10): # TO DO !!!!!\n","    epoch_loss = 0 \n","    for i, (x, y) in enumerate(zip(X, Y)):\n","\n","        x = torch.unsqueeze(x, 0) # TO DO !!!!!\n","        y = torch.unsqueeze(y, 0) # TO DO !!!!!\n","\n","        optimizer.zero_grad()\n","\n","        output = net(x ) # TO DO !!!!!\n","\n","        loss = nn.MSELoss()( output, y ) # TO DO !!!!!\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(\"Epoch {} - loss: {}\".format(epoch + 1, epoch_loss))\n","\n","\n","# let's test our model\n","# let's see if we can predict 0.5 * 0.5 = 0.25, 0.3 * 0.3 = 0.09, 0.2 * 0.2 = 0.04\n","X_test = torch.tensor([0.5, 0.3, 0.2])\n","Y_test = net(X_test.unsqueeze(1))\n","Y_test = Y_test.flatten().tolist()\n","Y_test = [round(y, 2) for y in Y_test]\n"]}]}