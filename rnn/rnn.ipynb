{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"14cOZqTBEXknvf6VIXNUd1ltuIe_7U5k5","authorship_tag":"ABX9TyOJwbrYJRqMHtN+A6aGs8Yl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":20,"metadata":{"id":"hp9QoauRUJAS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679518749475,"user_tz":420,"elapsed":3072,"user":{"displayName":"Kristian Rascon","userId":"13766589462812670013"}},"outputId":"ea4b04a4-cedf-4e50-871a-d37d7594014d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Training\n","0.9640683354582797\n"]}],"source":["#import torch and other necessary modules from torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","#import pandas and other necessary modules from pandas\n","import pandas as pd\n","\n","#import numpy and other necessary modules from numpy\n","import numpy as np\n","\n","#import sklearn and other necessary models from sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","\n","\n","# take a look at the csv file yourself first\n","# columns High, Low, Open are input features and column Close is target value\n","df = pd.read_csv('./coin_Bitcoin.csv') #For Gradescope: './coin_Bitcoin.csv' #'/content/drive/MyDrive/ASU Spring 2023/CSE 475/Labs/Lab3/Lab3pt2/coin_Bitcoin.csv'\n","x = df[[\"High\", \"Low\", \"Open\"]]\n","y = df[[\"Close\"]]\n","\n","# use StandardScaler from sklearn to standardize\n","scaler_x = StandardScaler()\n","scaler_y = StandardScaler()\n","x = scaler_x.fit_transform(x)\n","y = scaler_y.fit_transform(y)\n","\n","\n","# split into train and evaluation (8 : 2) using train_test_split from sklearn\n","train_x, test_x, train_y, test_y = train_test_split(x, y, train_size=0.8, test_size=0.2)\n","\n","\n","# now make x and y tensors, think about the shape of train_x, it should be (total_examples, sequence_lenth, feature_size)\n","# we wlll make sequence_length just 1 for simplicity, and you could use unsqueeze at dimension 1 to do this\n","# also when you create tensor, it needs to be float type since pytorch training do not take default type read using pandas\n","train_x = torch.tensor(train_x.reshape(-1, 1, train_x.shape[1])).float() #param -1: indicates to numpy to figure out that dim length\n","train_y = torch.tensor(train_y.reshape(-1, 1)).float()\n","test_x = torch.tensor(test_x.reshape(-1, 1, test_x.shape[1])).float()\n","seq_len = train_x[0].shape[0] # it is actually just 1 as explained above\n","\n","\n","# different from CNN which uses ImageFolder method, we don't have such method for RNN, so we need to write dataset class ourselves, reference tutorial is in main documentation\n","class BitCoinDataSet(Dataset):\n","    def __init__(self, train_x, train_y):\n","        super(Dataset, self).__init__()\n","        self.train_x = train_x\n","        self.train_y = train_y\n","\n","    def __len__(self):\n","        return len(self.train_x)\n","\n","    def __getitem__(self, idx):\n","        return self.train_x[idx], self.train_y[idx]\n","\n","\n","# now prepare dataloader for training set and evaluation set, and hyperparameters\n","hidden_size = 32 #between size of input and output: 2/3 the size of input layer +size of output layer and less than twice the size of input layer\n","num_layers = 2\n","learning_rate = 0.001\n","batch_size = 32 #default is 32: must be power of 2: 2,4,8,16,32,64,128\n","epoch_size = 10 #default is 10\n","\n","train_dataset = BitCoinDataSet(train_x, train_y)\n","test_dataset = BitCoinDataSet(test_x, test_y)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","# model design goes here\n","class RNN(nn.Module):\n","\n","    # there is no \"correct\" RNN model architecture for this lab either, you can start with a naive model as follows:\n","    # lstm with 5 layers (or rnn, or gru) -> linear -> relu -> linear\n","    # lstm: nn.LSTM (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n","\n","    def __init__(self, input_feature_size, hidden_size, num_layers):\n","        super(RNN, self).__init__()\n","        self.lstm = nn.LSTM(input_size=input_feature_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.lin1 = nn.Linear(in_features=hidden_size, out_features=64)\n","        self.relu = nn.ReLU()\n","        self.lin2 = nn.Linear(in_features=64, out_features=1)\n","    \n","    def forward(self, x):\n","        # Flatten, LSTM->Linear\n","        out, _ = self.lstm(x)\n","        out = self.lin1(out[:, -1, :])\n","        out = self.relu(out)\n","        out = self.lin2(out)\n","        return out\n","\n","\n","# instantiate your rnn model and move to device as in cnn section\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # whether your device has GPU\n","rnn = RNN(input_feature_size=x.shape[1], hidden_size=hidden_size, num_layers=num_layers).to(device)\n","# loss function is nn.MSELoss since it is regression task\n","criteria = nn.MSELoss()\n","# you can start with using Adam as optimizer as well \n","optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n","\n","\n","\n","# start training \n","rnn.train()\n","for epoch in range(epoch_size): # start with 10 epochs\n","\n","    loss = 0.0 # you can print out average loss per batch every certain batches\n","\n","    for batch_idx, data in enumerate(train_loader):\n","        # get inputs and target values from dataloaders and move to device\n","        inputs, targets = data\n","        inputs= inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # zero the parameter gradients using zero_grad()\n","        optimizer.zero_grad()\n","        # forward -> compute loss -> backward propogation -> optimize (see tutorial mentioned in main documentation)\n","        outputs = rnn(inputs)\n","        loss = criteria(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        loss += loss.item() # add loss for current batch\n","        if batch_idx % 100 == 99:    # print average loss per batch every 100 batches\n","            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {loss / 100:.3f}')\n","            loss = 0.0\n","\n","print('Finished Training')\n","\n","\n","\n","prediction = []\n","ground_truth = []\n","# evaluation\n","rnn.eval()\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, targets = data\n","        inputs = inputs.to(device)\n","\n","        ground_truth += targets.flatten().tolist()\n","        out = rnn(inputs).detach().cpu().flatten().tolist()\n","        prediction += out\n","\n","\n","# remember we standardized the y value before, so we must reverse the normalization before we compute r2score\n","prediction = np.array(prediction).reshape(-1,1)\n","prediction = scaler_y.inverse_transform(prediction) \n","ground_truth = np.array(ground_truth).reshape(-1,1)\n","ground_truth = scaler_y.inverse_transform(ground_truth)\n","\n","# use r2_score from sklearn\n","r2score = r2_score(ground_truth, prediction)\n","print(r2score)"]}]}