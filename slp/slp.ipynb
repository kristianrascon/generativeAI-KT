{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMtblmiWdB/mUApdZvDKzX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"I-SqtLzlt1Zx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712677341449,"user_tz":300,"elapsed":15333,"user":{"displayName":"Kristian Rascon","userId":"13766589462812670013"}},"outputId":"d87c5e69-015d-4b28-b832-c128a0ece85c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 - loss: 95.27497044065967\n","Epoch 2 - loss: 24.118282134499168\n","Epoch 3 - loss: 6.1615892666595755\n","Epoch 4 - loss: 1.577007510898511\n","Epoch 5 - loss: 0.40379727460367576\n","Epoch 6 - loss: 0.10340453466824329\n","Epoch 7 - loss: 0.02648137666034245\n","Epoch 8 - loss: 0.0067814261532817\n","Epoch 9 - loss: 0.0017367271625801095\n","Epoch 10 - loss: 0.0004447306725019473\n"]}],"source":["'''\n","We will use a single-layer perceptron in this lab to learn a simple linear function 3X + 1 = Y where X is a single dimension input variable and Y is a single dimension target variable.\n","Here `3` is weight `w` and `1` is bias `b`.\n","\n","We will use pytorch in this in class activity, you can use command 'pip install torch' to install torch on your local machine to develop your code or you can just use colab where is torch is pre-installed.\n","\n","The code needs you to implement is labeled by TO DO !!!!!, there is not much for you to implement but do learn about the entire workflow.\n","\n","Name your file slp.py when submitting to GradeScope.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","# for reproducibility, let's all use same random seed\n","torch.manual_seed(42)\n","\n","\n","# define a class that inherits nn.Module, and inside this class, you will write your model.\n","class Net(nn.Module):\n","\n","    # first method you must have is __init__() where various model layers are defined.\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Here you will need to define a single layer perceptron that takes in 1 dimensional input\n","        # variable and output 1 dimensional output variable.\n","        # nn.Linear(_,_ ): https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n","        self.fc = nn.Linear( 1, 1) # TO DO !!!!!\n","\n","    # second method you must have is forward() where you actually pass forward\n","    # your input features using the layers defined in __init__().\n","    def forward(self, x):\n","        # This function takes in input variables.\n","        # Actaully we can define what to pass in as x with Dataloader and batching,\n","        # which will be practiced in lab3. Because sometimes just knowing input features are\n","        # not enough for complex models to forward propogate. But this lab, we will simply pass in\n","        # input feature itself\n","\n","        # Here, just pass through x with self.fc\n","        out = self.fc(x)# TO DO !!!!!\n","\n","        # Now that we have out, just return this output of a single layer perceptron\n","        return out# TO DO !!!!!\n","\n","\n","# instantiate a model\n","net = Net()\n","\n","# define a optimizer, in this case, we will just use stochastic gradient descent\n","# with learning rate of 0.01.\n","# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n","optimizer = optim.SGD(net.parameters(), lr=0.01) # TO DO !!!!!\n","\n","\n","# let's create some data, since it is such a simple function, even with 100 data points,\n","# we will have a decent model, but remember this is not the case in real cases, a model rarely\n","# learns something with only 100 input data\n","X = 2 * torch.rand(100) - 1 # X within range of [-1, 1]\n","Y = X * 3.0 + 1\n","\n","\n","# start training, the following workflow is a standard way to training network using pytorch\n","# we will iterate through 10 epochs\n","for epoch in range(10): # TO DO !!!!!\n","    epoch_loss = 0\n","    for i, (x, y) in enumerate(zip(X, Y)):\n","\n","        # here x is tensor(1.) with dimension 0, y is tensor(4.) with dimension 0\n","        # however, nn.linear requires data to be at least dimension 1 (for supporting matrix multiplication and batching)\n","        # we will make tensor(1.) becomes tensor([1.]) first\n","        # you can use unsqueeze() function\n","        # https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n","        x = torch.unsqueeze(x, 0) # TO DO !!!!!\n","        y = torch.unsqueeze(y, 0) # TO DO !!!!!\n","\n","        # important: set gradient of all model parameters to 0 before calculating gradient\n","        # for current example, otherwise pytorch will accumulate current gradient\n","        # with previous gradient, this is not a behavior we want in our case\n","        optimizer.zero_grad()\n","\n","        # forward pass, just pass through x\n","        output = net(x) # TO DO !!!!!\n","\n","        # calculate loss using model output and target value -> between output and y\n","        # we will use nn.MSELoss()\n","        # https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n","        loss = nn.MSELoss()(output , y ) # TO DO !!!!!\n","\n","        # backward propogate\n","        loss.backward()\n","\n","        # update parameters with one optimization step\n","        optimizer.step()\n","\n","        # add up loss for current example to the total loss for current epoch\n","        epoch_loss += loss.item()\n","\n","    # print out some statistics\n","    print(\"Epoch {} - loss: {}\".format(epoch + 1, epoch_loss))\n","\n","\n","# now let's check what is learned\n","for name, param in net.named_parameters():\n","    if \"weight\" in name:\n","        w = round(param.item(), 1)\n","    if \"bias\" in name:\n","        b = round(param.item(), 1)\n","\n","\n"]}]}